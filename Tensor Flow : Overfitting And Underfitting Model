import tensorflow as tf
import keras
import matplotlib.pyplot as plt
import numpy as np
print(tf.__version__)
NUM_WORDS=10000

(train_data,train_labels),(test_data,test_labels)=keras.datasets.imdb.load_data(num_words=NUM_WORDS)

print("Train data before Muti-Hot Encoder: ", train_data[0])
#Create an all zero matrix of shape (len(sequences), dimension)
#converting the data in to a vector
def multi_hot_sequences(sequences,dimension):
              results=np.zeros((len(sequences),dimension))
              for i, word_indices in enumerate(sequences):
               results[i, word_indices]=1.0
               return results

train_data=multi_hot_sequences(train_data, dimension=NUM_WORDS)
test_data=multi_hot_sequences(test_data, dimension=NUM_WORDS)
print("Train data after Multi-Hot Encoder: ",train_data[0])


plt.plot(train_data[0])
plt.show()

# create a basline model

baseline_model=keras.Sequential(
          [keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(10000,)),
           keras.layers.Dense(16, activation=tf.nn.relu),
           keras.layers.Dense(1,activation=tf.nn.sigmoid)
           ]
)

baseline_model.compile(optimizer='adam',
                       loss='binary_crossentropy',
                       metrics=['accuracy', 'binary_crossentropy'])
baseline_model.summary()
print("Baseline Model: ")
baseline_history=baseline_model.fit(train_data,
                                    train_labels,
                                    epochs=20,
                                    batch_size=512,
                                    validation_data=(test_data,test_labels),
                                    verbose=2)


#Create a smaller model
smaller_model=keras.Sequential(
     [keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(10000,)),
      keras.layers.Dense(4, activation=tf.nn.relu),
      keras.layers.Dense(1, activation=tf.nn.sigmoid)
     ]
)

smaller_model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy', 'binary_crossentropy'])

smaller_model.summary()

print("Result of the samller model: ")
smaller_history=smaller_model.fit(train_data,
                                  train_labels,
                                  epochs=20,
                                  validation_data=(test_data,test_labels),
                                  verbose=2)


# Create a bigger model
bigger_model=keras.models.Sequential(
    [keras.layers.Dense(512,activation=tf.nn.relu, input_shape=(10000,)),
     keras.layers.Dense(512,activation=tf.nn.relu),
     keras.layers.Dense(1,activation=tf.nn.sigmoid)
    ]
)

bigger_model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy', 'binary_crossentropy'])
bigger_model.summary()

bigger_history=bigger_model.fit( train_data,
                                 train_labels,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(test_data,test_labels),
                                 verbose=2
                                 )


# Plot the training and validation loss
def plot_history(histories, key='binary_crossentropy'):
    plt.figure(figsize=(16,10))

    for name, history in histories:
        val=plt.plot(history.epoch, history.history['val_'+key],
                     '--', label=name.title()+'Val')
        plt.plot(history.epoch, history.history[key],
                 color=val[0].get_color(),
                 label=name.title()+'Train')

    plt.xlabel('Epochs')
    plt.ylabel(key.replace('_',' ').title())
    plt.legend()

    plt.xlim([0,max(history.epoch)])

plot_history([('baseline', baseline_history),
             ('smaller',smaller_history),
             ('bigger', bigger_history)])
plt.show()


# Apply regularization to avoid the overftting of the model
# L2 Regularization (add weights as a cost of loss function will be as that of square of absolute weight)

l2_model=keras.models.Sequential([
    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation=tf.nn.relu, input_shape=(10000,)),
    keras.layers.Dense(16,kernel_regularizer=keras.regularizers.l2(0.001),
                       activation=tf.nn.relu),
    keras.layers.Dense(1,activation=tf.nn.sigmoid)

])



l2_model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy','binary_crossentropy'])

l2_model_history=l2_model.fit(train_data,train_labels,
                              epochs=20,
                              batch_size=512,
                              validation_data=(test_data,test_labels),
                              verbose=2)

#l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value to the total loss of the network. Note that because this penalty is only added at training time, the loss for this network will be much higher at training than at test time.


#plot

plot_history([('baseline', baseline_history),
                'l2',l2_model_history])
plt.show()


# Dropout

dpt_model=keras.models.Sequential([
    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(10000,)),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])


dpt_model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy', 'binary_crossentropy'])

dpt_model_history=dpt_model.fit(train_data, train_labels,
                                epochs=20,
                                batch_size=512,
                                validation_data=(test_data, test_labels),
                                verbose=2)

#plot

plot_history([('baseline',baseline_history),
              'dropout',dpt_model_history])
plt.show()
